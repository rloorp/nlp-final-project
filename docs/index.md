## NLP Final project

## Introduction
In this project, the goal is to leverage the Enron Email Dataset to generate fictional stories based on the existing context within the emails. The aim is to extract meaningful insights from the email interactions and transform them into coherent narratives.

## Background
Enron Corporation was an American energy, commodities, and services company based in Houston, Texas. The Enron scandal in 2001 led to one of the largest bankruptcies in U.S. history and the collapse of the auditing firm Arthur Andersen. Enron, once a dominant energy trader, engaged in fraudulent accounting practices to hide debt and inflate profits. When the fraud was uncovered, the company's stock collapsed, leading to massive losses for employees and investors. The scandal resulted in several criminal convictions and led to the Sarbanes-Oxley Act, which reformed corporate governance and financial reporting practices.

## Dataset
The Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.

For this project we used a cleaned version of the original enron dataset. This version cleans and extracts relevant fields from the Enron emails, and saves the processed information in manageable batches, making it easier to work with large-scale email data.

It was decided to work with this cleaned dataset because at the beginning, the preprocessing and data cleaning was attempted with the complete dataset, but it was very large and the execution took a long time.

## Models
### GPT-2 
GPT-2 is a transformer model pretrained on a large English corpus in a self-supervised manner, meaning no human-labeled data was used. It was trained to predict the next word in a sequence of text. The model uses a masking mechanism to ensure each prediction only considers previous words and not future ones. Through this process, GPT-2 learns internal representations of the English language, which can be useful for various tasks. However, its strength lies in generating text from a given prompt, which is what it was primarily trained for.

### Distilbert
DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than google-bert/bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.

## Experiment
### Dataset Loading and Preprocessing
The email dataset is loaded from Google Drive using pandas. An initial inspection of the data is performed to verify correct loading. A function is applied to clean the email text field by removing quotes and lists that might interfere with processing. Emails shorter than 30 characters are filtered out to avoid irrelevant analysis, and common signatures like "Thank you" or "Best regards" are removed. This ensures that only the most relevant content remains for analysis.

### Tokenization and Stopword Removal
Using the NLTK library, tokenization is performed, breaking down the emails into individual words. Stopwords (common words like "the," "is," etc.) that don't add significant meaning are then removed. This cleaning step is crucial to reduce noise in the model and focus on the keywords that define the content of the emails.

### Topic Modeling with Latent Dirichlet Allocation (LDA)
To extract latent topics from the emails, a Latent Dirichlet Allocation (LDA) model is employed, organizing the emails into 5 primary topics. The text is vectorized using CountVectorizer, transforming the words into a numerical matrix. LDA then groups the most frequent words within each topic, allowing for an interpretation of the recurring themes in the dataset.

### Embeddings with DistilBERT
Once the emails are preprocessed, the DistilBERT model is used to obtain embeddings of the emails. These embeddings are numerical representations of the meaning of the emails and are processed in batches using a GPU to speed up computation. The embeddings are derived from the last hidden layer of DistilBERT, averaging all the words in the email. This allows the full semantics of each email to be captured in a feature vector.

### Clustering of Emails
With the embeddings generated, the K-Means algorithm is applied to cluster the emails into 5 groups, each representing a topic or set of emails with similar semantics. K-Means helps identify hidden patterns and groups of emails that share common themes. In the end, a few emails from each cluster are displayed, making it easier to manually analyze the results.

### Interaction Analysis between Senders and Recipients
The code also groups the emails by sender and recipient, showing the most frequent interactions between employees. This analysis provides insight into the most active work relationships within the company, which can be useful for understanding team dynamics.

### Text Generation with GPT-2
Finally, the GPT-2 language model is used to generate creative text. Starting from an initial prompt based on an office romance scenario, the model generates a continuation of the story. This is done by tokenizing the initial prompt, feeding it into the model, and decoding the generated response. Using GPT-2 allows you to create a fictional story based on the context and characters extracted from the emails, fulfilling the project’s goal of using natural language processing to generate narratives based on real-world data.

## Results

A story about the interactions between employees in an office begins... 

The first thing that happens is a lot of people start to talk and then they get very angry at each other for not doing something wrong with their colleagues' lives... 

I think it's important we all understand what this means when you're working together as one company because if there are things going well but your boss doesn't do them properly he can be fired from his job immediately after having done some bad stuff.... 

It makes sense why so many companies have these problems.... 

If someone has been involved before getting hired by another person who was also part owner etc., how does anyone know where those relationships ended? And even though most managers don`T want any kind'relationship', sometimes just being around others will help keep everyone happy.. So let me ask my question.. What would happen without management? How could such situations occur?? Would our team members feel like "I am here" while still managing us?? Is every employee really responsible??? We should always take care of ourselves! But maybe no matter which way somebody goes out into business..... 

Maybe nobody ever gets paid enough money!!

## Conclusions

Using models such as distilbert and gpt-2 for text generation based on corporate emails yields promising results, however, there are shortcomings when it comes to recognizing the content of the email, which is why it is recommended to further analyze the data cleaning algorithms.

## References
- openai-community/gpt2 · Hugging Face. (n.d.). https://huggingface.co/openai-community/gpt2
- DistilBERT. (n.d.). https://huggingface.co/docs/transformers/model_doc/distilbert
- Bondarenko, P. (2024, July 29). Enron scandal | Summary, Explained, History, & Facts. Encyclopedia Britannica. https://www.britannica.com/event/Enron-scandal
- The Enron email Dataset. (2016, June 16). Kaggle. https://www.kaggle.com/datasets/wcukierski/enron-email-dataset
- Enron Clean dataset. (2020, June 15). Kaggle. https://www.kaggle.com/datasets/amank56/enron-clean-dataset/data
